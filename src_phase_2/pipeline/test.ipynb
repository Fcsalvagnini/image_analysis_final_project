{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'compare.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_fwf(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00002649_00000001_slap_S_01.png</th>\n",
       "      <th>00002649_00000001_roll_U_01.png</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002649_00000001_roll_V_01.png</td>\n",
       "      <td>00002312_00000001_roll_U_01.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002649_00000001_roll_U_01.png</td>\n",
       "      <td>00002649_00000001_roll_V_01.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00002649_00000001_roll_V_01.png</td>\n",
       "      <td>00002649_00000001_slap_S_01.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00002312_00000001_roll_U_01.png</td>\n",
       "      <td>00002649_00000001_slap_S_01.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   00002649_00000001_slap_S_01.png  00002649_00000001_roll_U_01.png\n",
       "0  00002649_00000001_roll_V_01.png  00002312_00000001_roll_U_01.png\n",
       "1  00002649_00000001_roll_U_01.png  00002649_00000001_roll_V_01.png\n",
       "2  00002649_00000001_roll_V_01.png  00002649_00000001_slap_S_01.png\n",
       "3  00002312_00000001_roll_U_01.png  00002649_00000001_slap_S_01.png"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(txt, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.reset_index of                                                                5\n",
       "00002649_00000001_slap_S_01.png  00002649_00000001_roll_U_01.png\n",
       "00002649_00000001_roll_V_01.png  00002312_00000001_roll_U_01.png\n",
       "00002649_00000001_roll_U_01.png  00002649_00000001_roll_V_01.png\n",
       "00002649_00000001_roll_V_01.png  00002649_00000001_slap_S_01.png\n",
       "00002312_00000001_roll_U_01.png  00002649_00000001_slap_S_01.png>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_dataframe(txtPath, sep=' '):\n",
    "    df = pd.read_csv(txtPath, sep=sep)\n",
    "    df = df.reset_index()\n",
    "    df.columns = ['image1', 'image2']\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_txt_dataframe(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "split = kf.split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nakano/.virtualenvs/brickbanker/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/nakano/.virtualenvs/brickbanker/lib/python3.8/site-packages/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022\n",
      "  warnings.warn(\"pyprof will be removed by the end of June, 2022\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_b0-c7cc451f.pth\" to /home/nakano/.cache/torch/hub/checkpoints/tf_efficientnetv2_b0-c7cc451f.pth\n"
     ]
    }
   ],
   "source": [
    "encoder = timm.create_model(model_name='tf_efficientnetv2_b0', \n",
    "                                    pretrained=True,\n",
    "                                    features_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_feature = encoder(torch.randn(1, 3, 224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = 1\n",
    "for dim in out_feature[-1].shape:\n",
    "    n_out *= dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9408"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_reductor():\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(n_out, 512), nn.ReLU(inplace = True),\n",
    "        nn.Linear(512, 256), nn.ReLU(inplace=True),\n",
    "        nn.Linear(256, 64)\n",
    "    )\n",
    "    \n",
    "def forward(input1, input2):\n",
    "    output1 = encoder(input1)[-1]\n",
    "    output1 = dimensionality_reductor()(output1)\n",
    "    output2 = encoder(input2)[-1]\n",
    "    output2 = dimensionality_reductor()(output2)\n",
    "\n",
    "    return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1, i2 = torch.randn(3, 3, 224, 224), torch.randn(3, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1, o2 = forward(i1, i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FeatureListNet' object has no attribute 'reset_classifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/nakano/unicamp/MO445_P2/image_analysis_final_project/src_phase_2/pipeline/test.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/nakano/unicamp/MO445_P2/image_analysis_final_project/src_phase_2/pipeline/test.ipynb#ch0000013?line=0'>1</a>\u001b[0m encoder\u001b[39m.\u001b[39;49mreset_classifier(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/brickbanker/lib/python3.8/site-packages/torch/nn/modules/module.py:1177\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   <a href='file:///home/nakano/.virtualenvs/brickbanker/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1174'>1175</a>\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   <a href='file:///home/nakano/.virtualenvs/brickbanker/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1175'>1176</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> <a href='file:///home/nakano/.virtualenvs/brickbanker/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1176'>1177</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   <a href='file:///home/nakano/.virtualenvs/brickbanker/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1177'>1178</a>\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FeatureListNet' object has no attribute 'reset_classifier'"
     ]
    }
   ],
   "source": [
    "encoder.reset_classifier(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64, 128, 256, 512, 512, 512]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.feature_info.channels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 7, 7])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoder.eval()\n",
    "encoder(torch.randn(1, 3, 224, 224))[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25088"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512*7*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "imPath = '../registered_images_v1'\n",
    "imList = os.listdir(imPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "im = cv2.imread(os.path.join(imPath, imList[0]))\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90, Perspective,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue, Affine,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, PiecewiseAffine, RandomResizedCrop,\n",
    "    Sharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout,\n",
    "    ShiftScaleRotate, CenterCrop, Resize, ElasticTransform, ImageCompression,\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "        Resize(224, 224, interpolation=cv2.INTER_CUBIC, p=1.),\n",
    "        # Transpose(p=0.5),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        VerticalFlip(p=0.5),\n",
    "        # ShiftScaleRotate(p=0.5),\n",
    "        # Perspective(p=0.5),\n",
    "        #ElasticTransform(p=0.5),\n",
    "        #GridDistortion(p=0.5),\n",
    "        #CLAHE(p=0.5),\n",
    "        #Cutout(p=0.25),\n",
    "        #MotionBlur(p=0.25),\n",
    "        #ImageCompression(p=0.5, quality_lower=50, quality_upper=100),\n",
    "        # Affine(scale=[0.5, 1.5], p=0.5),\n",
    "        Sharpen(p=0.25),\n",
    "        #HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "        #RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "        #CoarseDropout(p=0.5),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], p=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "_im = get_train_transforms()(image=im)['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_im = _im.unsqueeze(0)\n",
    "_im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 7, 7])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(_im)[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in encoder.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 7, 7])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.eval()\n",
    "encoder(torch.randn(1, 3, 224, 224))[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_chs': 512, 'reduction': 32, 'module': 'features.36'}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.feature_info[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64, 128, 256, 512, 512, 512]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.feature_info.channels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkTimmBackbone(nn.Module):\n",
    "    def __init__(self, model_name:str):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model(model_name=model_name, \n",
    "                                       pretrained=True,\n",
    "                                       features_only=True)\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        n_out = self.encoder.feature_info[-1]['num_chs']\n",
    "\n",
    "        self.dimensionality_reductor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(25088, 512), nn.ReLU(inplace = True),\n",
    "            nn.Linear(512, 256), nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 64)\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        print(input1.shape)\n",
    "        print(input1.dtype)\n",
    "        \n",
    "        output1 = self.encoder(input1)[-1]\n",
    "        print(output1.shape)\n",
    "        output1 = self.dimensionality_reductor(output1)\n",
    "        output2 = self.encoder(input2)[-1]\n",
    "        output2 = self.dimensionality_reductor(output2)\n",
    "\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = SiameseNetworkTimmBackbone('vgg19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.float32\n",
      "torch.Size([1, 512, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.3377,  0.7286,  1.5252,  2.7763, -5.9279,  2.1389, -1.2336,  1.2438,\n",
       "           0.3906,  3.6307, -1.7119,  0.0957, -0.5849, -0.9776,  0.9360, -2.1718,\n",
       "           1.4305, -3.3072,  2.9513, -2.1385, -2.0622, -0.8845, -0.9819, -0.0067,\n",
       "          -0.9565,  1.3756,  2.0640, -1.2633, -1.0078, -0.3105, -2.1146, -0.6408,\n",
       "          -1.8333,  4.9663,  1.3228,  1.0289,  2.6461,  0.1993,  2.0960, -2.8636,\n",
       "          -1.0461,  0.8179, -0.4607,  2.4057, -2.1714, -0.0397, -0.3311, -0.8036,\n",
       "           0.5419, -0.1144,  1.0472,  0.5534, -0.3284, -1.2199,  0.8545,  1.7818,\n",
       "          -1.2975,  1.1899,  0.8478, -0.8254, -0.4918,  0.7887,  4.6003,  2.7350]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[ 1.3377,  0.7286,  1.5252,  2.7763, -5.9279,  2.1389, -1.2336,  1.2438,\n",
       "           0.3906,  3.6307, -1.7119,  0.0957, -0.5849, -0.9776,  0.9360, -2.1718,\n",
       "           1.4305, -3.3072,  2.9513, -2.1385, -2.0622, -0.8845, -0.9819, -0.0067,\n",
       "          -0.9565,  1.3756,  2.0640, -1.2633, -1.0078, -0.3105, -2.1146, -0.6408,\n",
       "          -1.8333,  4.9663,  1.3228,  1.0289,  2.6461,  0.1993,  2.0960, -2.8636,\n",
       "          -1.0461,  0.8179, -0.4607,  2.4057, -2.1714, -0.0397, -0.3311, -0.8036,\n",
       "           0.5419, -0.1144,  1.0472,  0.5534, -0.3284, -1.2199,  0.8545,  1.7818,\n",
       "          -1.2975,  1.1899,  0.8478, -0.8254, -0.4918,  0.7887,  4.6003,  2.7350]],\n",
       "        grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm(_im, _im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config_yaml(config_path):\n",
    "    with open(config_path, \"r\") as yaml_file:\n",
    "        config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config_yaml('config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def config_flatten(config, fconfig):\n",
    "    for key in config:\n",
    "        if isinstance(config[key], dict):\n",
    "            fconfig = config_flatten(config[key], fconfig)\n",
    "        else:\n",
    "            fconfig[key] = config[key]\n",
    "    return fconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "fconfig = {}\n",
    "fconfig = config_flatten(config, fconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_path': 'registred_images_v2',\n",
       " 'data_nlets': 'compare_fp_v1.txt',\n",
       " 'network': 'tf_efficientnetv2_b0',\n",
       " 'train_batch': 128,\n",
       " 'valid_batch': 32,\n",
       " 'num_workers': 1,\n",
       " 'image_size': 224,\n",
       " 'nchannels': 3,\n",
       " 'num_splits': 5,\n",
       " 'epochs': 10,\n",
       " 'lr': 0.001,\n",
       " 'scheduler': 'stepLr',\n",
       " 'accum_iter': 1,\n",
       " 'verbose_step': 1,\n",
       " 'margin': 2.0,\n",
       " 'contrastive_thresh': 1.1}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = 'ADD'\n",
    "toadd = 10\n",
    "y = [style for i in range(toadd)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADD', 'ADD', 'ADD', 'ADD', 'ADD', 'ADD', 'ADD', 'ADD', 'ADD', 'ADD']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21adaf4e9b67bc7c9d6e5a2652a6184e56f56841d18ea464ff2a8d182b8c9e34"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('brickbanker')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
